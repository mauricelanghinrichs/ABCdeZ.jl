var documenterSearchIndex = {"docs":
[{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"CurrentModule = ABCdeZ","category":"page"},{"location":"#ABCdeZ.jl","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Approximate Bayesian Computation (ABC) with differential evolution (de) moves and model evidence (Z) estimates.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"ABCdeZ.jl offers Bayesian parameter estimation and model comparison/selection for inference problems with an intractable likelihood. Models only need to be simulated (instead of calculating the likelihood). In this documentation you will find everything to get started.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"ABCdeZ.jl was developed @TSB by Maurice Langhinrichs and Nils Becker. This work is based on many people's previous achievements, particular some part of the code base was adapted from KissABC.jl [1]; please find a complete list of references below.","category":"page"},{"location":"#Brief-introduction","page":"ABCdeZ.jl","title":"Brief introduction","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Bayesian inference allows to update a parameter prior p(θ  M) in the light of new data D to obtain posterior knowledge as","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"p(θ  D M) = fracp(D θ M)  p(θ  M)Z","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"where θ are the parameters of model M, p(D θ M) the likelihood and Z = p(D  M) is the model evidence (also known as marginal likelihood). The model evidence is the normalising integral ","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Z equiv p(D  M) = int  p(D θ M)  p(θ  M)  mathrmdθ","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Next to parameter estimation, Bayes' theorem also applies to the outer layer for model selection/comparison, updating (by the same data) model prior probabilities p(M) to posterior probabilities as ","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"p(M  D) = fracp(D  M)  p(M)p(D)","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"where the model evidences reappear as the essential factors, and p(D) = sum_i p(D  M_i)  p(M_i) is a normalisation factor over all models M_i (i = 1n).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"ABC (Approximate Bayesian Computation) approximates the likelihood through model simulations [2], yielding simulated data D that are evaluated with a kernel π_ϵ(D  D),","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"hatp(D θ M) = int  p(D  θ M)  π_ϵ(D  D)  mathrmdD","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"where ϵ  0 specifies the width of the kernel. Under certain conditions, it can be shown that this approximation converges for ϵ rightarrow 0 [2], keeping only model simulations D that are \"very close\" to the original data D.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"These equations show in brief, that posterior parameter and posterior model probabilities can be obtained by updating (model and parameter) priors through some data D, only requiring model simulations and a sufficiently small target ϵapprox 0. ","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"ABCdeZ.jl will provide parameter samples approximating the posterior p(θ  D M) and estimations of (logarithmic) model evidences mathrmlogZ.","category":"page"},{"location":"#ABCdeZ.jl-2","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"ABCdeZ.jl currently implements two main ABC methods:","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"abcdesmc! (from \"abc de smc\"): A Sequential Monte Carlo algorithm. For posterior samples and model evidence estimates. Method of choice for multimodal or complex posterior landscapes.\nabcdemc! (from \"abc de mc\"): A simpler MCMC algorithm. For posterior samples only. \"Greedy\" algorithm, that may be faster than abcdesmc! for some problems. May have difficulties with multimodal problems (use abcdesmc! instead).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Both methods make differential evolution (de) proposals for the MCMC (Markov chain Monte Carlo) steps of the algorithms; differential evolution provides a good scale and orientation to explore the (often) complex multi-dimensional parameter spaces effectively [4]. For more information on both algorithms follow the respective links above.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Why ABCdeZ.jl?","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"If you want to perform parameter estimation and/or model comparison, but it is difficult to compute the likelihood for your models (time-consuming and/or analytically unknown). ABC is a likelihood-free approach, the only requirement is that you can simulate your models, given values for their parameters.\nABCdeZ.jl allows to compute model evidences directly and hence the results are future-proof. Model selection in ABC is often done by simulating all models of interest in the same ABC run. With ABCdeZ.jl you can run models individually, store the resulting evidence (and ϵ's) and compute evidences of any other models later, without re-computing the first. In the end, the desired set of models can be compared by transforming their evidences into posterior model probabilities or Bayes factors.\nABCdeZ.jl offers fast thread-based parallelism by enabled by FLoops.jl [10]; additional it also allows to store arbitrary data blobs together with the sample particles (e.g., to have the simulation output alongside the final posterior samples).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Why not ABC(deZ.jl)?","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"If you can calculate the likelihood function for your models, make use of it. Instead of ABC, consider likelihood-based inference methods.\nPlease be aware of the caveats of ABC, particular for model comparison/selection. ABC is approximative and may lead to wrong/misleading results if not applied carefully (see here for more).","category":"page"},{"location":"#Minimal-example","page":"ABCdeZ.jl","title":"Minimal example","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Here we will use ABCdeZ.jl to infer the model evidences and posterior distributions within a simple toy example. The models have a single parameter θ that is, a priori, normally distributed (prior). After seeing one single data point, we obtain an updated posterior knowledge. To obtain model evidences, next to the posterior samples, we will use the abcdesmc! method; for posterior samples only one may also use abcdemc! with a very similar syntax (as seen here).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"ABCdeZ.jl (as ABC in general) requires model simulations (for samples of θ) only, and not the likelihood values (at these θ). However, in this simple example the likelihood is available and chosen in a way (also Normal) that the prior is conjugate, and the exact posterior distributions and evidences are known analytically. We will compare the inferences made by ABCdeZ.jl with the analytical counterparts.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"We load the required packages, set the single data point and the target ϵ for the ABC runs.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"using ABCdeZ\nusing Distributions\n\n### data\ndata = 3\n\n### ABC target ϵ (maximum distance)\nϵ = 0.3","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Then we set up the inference of a first model. The normal prior is specified via Distributions (for more see \"Prior\" box below). Note that the model is solely specified by a  random simulation for a given θ. The distance function here simply reports the absolute distance between the random model output and the single data point (see here for additional features in the distance function).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"### model 1 inference\nσ₁² = 10\nprior1 = Normal(0, sqrt(σ₁²))\n\n# model simulation (to replace likelihood)\nmodel1(θ) = rand(Normal(θ, 1))\n\n# distance function between model and data\ndist1!(θ, ve) = abs(model1(θ)-data), nothing","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"With the following line we run the abcdesmc! method of ABCdeZ.jl. The inference result is stored in r1.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"### ABC run\n# run the smc method for model 1\nr1 = abcdesmc!(prior1, dist1!, ϵ, nothing, \n                    nparticles=1000, parallel=true)","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"From the result r1 we can read out the posterior samples (important: weighted by Wns,  see box \"Posterior sample weights\" here) and the estimated model evidence for model 1 (transforming back from log-scale).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"### process results\n# posterior parameters\nposterior1 = [t[1] for t in r1.P[r1.Wns .> 0.0]]\n\n# model evidence (logZ is the logarithmic evidence)\nevidence1 = exp(r1.logZ)","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Plotting the posterior1 samples as a histogram, one can see the gained knowledge from the  prior just by the single data point (Figure below, left panel). The samples also match the analytical posterior, available in this simple example.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"<img src=\"./assets/abcdez_min_ex_post.png\" width=\"566\">","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"To demonstrate model comparison enabled by ABCdeZ.jl, we now repeat the procedure with a  second model. Here, for simplicity, model 2 only differs from model 1 in the prior; of course  in more interesting settings, not only the prior, but the whole architecture of models  may be different (kind of and/or number of parameters). Any models can be compared in principle;  as long as inferences are done for the same (summary) data (and simulations always match the structure of the data), distance method (here abs()) and  target ϵ. The same target ϵ is important in ABCdeZ.jl due to the (typically) unnormalised  ABC kernel (see box \"Model evidence (off by a factor)\" here; if not possible, advanced info here).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"### model 2 inference\nσ₂² = 100\nprior2 = Normal(0, sqrt(σ₂²))\n\nmodel2(θ) = model1(θ)\n\ndist2!(θ, ve) = abs(model2(θ)-data), nothing\n\nr2 = abcdesmc!(prior2, dist2!, ϵ, nothing, \n                    nparticles=1000, parallel=true)\n\nposterior2 = [t[1] for t in r2.P[r2.Wns .> 0.0]]\nevidence2 = exp(r2.logZ)","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"The posterior inference of model 2 is visually very similar to model 1, except the difference in the prior (Figure above, right panel).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Finally, the estimated evidences can be used to compute posterior model probabilities. The model prior is uniform between the two models here.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"### model probabilities\n# model priors (uniform here)\nmprior1 = 0.5\nmprior2 = 0.5\n\n# model posterior probabilities\nmposterior1 = evidence1*mprior1 / (evidence1*mprior1 + evidence2*mprior2) # posterior prob. model 1\nmposterior2 = evidence2*mprior2 / (evidence1*mprior1 + evidence2*mprior2) # posterior prob. model 2","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"The model probabilities can be visually compared (Figure below), recovering the exact analytical results. Note that evidence values by ABCdeZ.jl are numerically uncertain (see box \"Uncertainty of model evidence\" here); as such the figure below also shows evidence values from repeated runs.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"<img src=\"./assets/abcdez_min_ex_model_sel.png\" width=\"320\">","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"The complete code for this minimal example, including the derivation of the  analytical counterparts, can be found on GitHub.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"note: Prior\nMultidimensional prior distributions (continuous, discrete or mixed) can  be specified via the Factored() syntax (from  independent 1d marginals), e.g. prior2d = Factored(Normal(0, sqrt(10)),  DiscreteUniform(1, 10)).","category":"page"},{"location":"#Inference-by-abcdesmc!","page":"ABCdeZ.jl","title":"Inference by abcdesmc!","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"abcdesmc!","category":"page"},{"location":"#ABCdeZ.abcdesmc!","page":"ABCdeZ.jl","title":"ABCdeZ.abcdesmc!","text":"abcdesmc!(prior, dist!, ϵ_target, varexternal; <keyword arguments>)\n\nRun ABC with diffential evolution (de) moves in a sequential Monte Carlo setup (smc)  providing posterior samples and a model evidence estimate.\n\nThe particles have to be weighted (via r.Wns) for valid posterior samples.\n\nArguments\n\nprior: Distribution or Factored object specifying the parameter prior.\ndist!: distance function computing the distance (≥ 0.0) between model and data,    for given (θ, ve) input (θ parameters, ve external variables, see varexternal).\nϵ_target: final target distance (or more general, target width of the ABC kernel); algorithm    stops if ϵ_target or nsims_max is reached.\nvarexternal: external variables that are passed as second positional argument to dist!    and can be used to support the distance computation with fast in-place operations in    a thread-safe manner; objects in varexternal can be in-place mutated, even in parallel mode,    as each thread will receive its own copy of varexternal (if not needed input nothing).\nnparticles::Int=100: number of total particles to use for inference.\nα=0.95: used for adaptive choice of ϵ specifying the sequential target distributions; technically,    ϵ will be the α-quantile of current particle distances.\nδess=0.5: if the fractional effective sample size drops below δess, a stratified resampling step is performed.\nnsims_max::Int=10^7: maximal number of dist! evaluations (not counting initial samples from prior);    algorithm stops if ϵ_target or nsims_max is reached.\nKmcmc::Int=3: number of MCMC (Markov chain Monte Carlo) steps at each sequential    target distribution specified by current ϵ and ABC kernel type.\nABCk=ABCdeZ.Indicator0toϵ: ABC kernel to be specified by ϵ widths that receives distance values.\nfacc_min=0.25: if the fraction of accepted MCMC proposals drops below facc_min, diffential evolution    proposals are reduced by a factor of facc_tune.\nfacc_tune=0.95: factor to reduce the jump distance of the diffential evolution    proposals in the MCMC step (used if facc_min is reached).\nverbose::Bool=true: if set to true, enables verbosity (printout to REPL).\nverboseout::Bool=true: if set to true, algorithm returns a more detailed inference output.\nrng=Random.GLOBAL_RNG: an AbstractRNG object which is used by the inference.\nparallel::Bool=false: if set to true, threaded parallelism is enabled; dist! must be    thread-safe in such a case, e.g. by making use of varexternal (ve).\n\nExamples\n\njulia> using ABCdeZ, Distributions;\njulia> data = 5;\njulia> prior = Normal(0, sqrt(10));\njulia> model(θ) = rand(Normal(θ, 1));\njulia> dist!(θ, ve) = abs(model(θ)-data), nothing;\njulia> ϵ = 0.3;\njulia> r = abcdesmc!(prior, dist!, ϵ, nothing, nparticles=1000, parallel=true);\njulia> posterior = [t[1] for t in r.P[r.Wns .> 0.0]];\njulia> evidence = exp(r.logZ);\n\n\n\n\n\n","category":"function"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"warning: Posterior sample weights\nPosterior samples obtained by abcdesmc! have to be associated with their weights (r.Wns).  With an indicator ABC kernel (default) there are just two weights (i.e. alive and dead  particles) and the correct posterior samples are hence given by  posterior = [t[1] for t in r.P[r.Wns .> 0.0]] (for the first parameter here).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"warning: Model evidence (off by a factor)\nThe model evidence estimates from the abcdesmc! method obtained by the default ABC  indicator kernel are off by a normalisation factor coming from an unnormalised  kernel (the one used in the final iteration). To do model selection / comparison  this means that evidence estimates for the set of models have to be done for the same  data (or summary statistics), distance function, ABC kernel and the same target  ϵ (which is ϵ_target if run not stopped by nsims_max). Then the (unknown)  normalisation factor is the same for all models and does not matter (cancels) for  Bayes factors or posterior model probabilities. See here for workarounds if ϵ is not the same.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"tip: Uncertainty of model evidence\nAs of now the abcdesmc! method does not provide a (numerical) uncertainty for the model evidence  estimate from a single run. It may be however very useful to check for this when doing  model comparison (as the resulting Bayes factors or posterior model probabilities are  uncertain as well). So, if the runtime permits, run the abcdesmc! method multiple times  and collect the resulting set of evidence values for mean/median and std estimates.","category":"page"},{"location":"#Inference-by-abcdemc!","page":"ABCdeZ.jl","title":"Inference by abcdemc!","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"abcdemc!","category":"page"},{"location":"#ABCdeZ.abcdemc!","page":"ABCdeZ.jl","title":"ABCdeZ.abcdemc!","text":"abcdemc!(prior, dist!, ϵ_target, varexternal; <keyword arguments>)\n\nRun ABC with diffential evolution (de) moves in a Markov chain Monte Carlo setup (mc)  providing posterior samples.\n\nAlgorithm needs to converge for an unbiased posterior estimate.\n\nArguments\n\nprior: Distribution or Factored object specifying the parameter prior.\ndist!: distance function computing the distance (≥ 0.0) between model and data,    for given (θ, ve) input (θ parameters, ve external variables, see varexternal).\nϵ_target: final target distance (or more general, target width of the ABC kernel); algorithm    equilibrates to final target distribution (approximate posterior) if ϵ_target is reached.\nvarexternal: external variables that are passed as second positional argument to dist!    and can be used to support the distance computation with fast in-place operations in    a thread-safe manner; objects in varexternal can be in-place mutated, even in parallel mode,    as each thread will receive its own copy of varexternal (if not needed input nothing).\nnparticles::Int=50: number of total particles to use for inference in each generation.\ngenerations::Int=20: number of generations (total iterations) to run the algorithm.\nverbose::Bool=true: if set to true, enables verbosity (printout to REPL).\nrng=Random.GLOBAL_RNG: an AbstractRNG object which is used by the inference.\nparallel::Bool=false: if set to true, threaded parallelism is enabled; dist! must be    thread-safe in such a case, e.g. by making use of varexternal (ve).\n\nExamples\n\njulia> using ABCdeZ, Distributions;\njulia> data = 5;\njulia> prior = Normal(0, sqrt(10));\njulia> model(θ) = rand(Normal(θ, 1));\njulia> dist!(θ, ve) = abs(model(θ)-data), nothing;\njulia> ϵ = 0.3;\njulia> r = abcdemc!(prior, dist!, ϵ, nothing, nparticles=1000, generations=300, parallel=true);\njulia> posterior = [t[1] for t in r.P];\n\n\n\n\n\n","category":"function"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"warning: Greediness of abcdemc!\nThe abcdemc! method implements a \"greedy\"/biased Metropolis-Hasting step in the  Markov chain. This allows a fast convergence, particularly well-suited for unimodal  problems. However, to obtain valid posterior estimates the algorithm needs to  converge (all particles below ϵ_target and r.reached_ϵ==true). Otherwise the samples will be  biased (closer to the MAP (maximum a posteriori probability) parameter values  with reduced variation).","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"note: Note\nIn contrast to the abcdesmc! method the resulting posterior samples of the abcdemc!  method are not associated with weights and can be used directly, i.e.  posterior = [t[1] for t in r.P] (for the first parameter here).","category":"page"},{"location":"#Distributions-and-Priors","page":"ABCdeZ.jl","title":"Distributions and Priors","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Factored","category":"page"},{"location":"#ABCdeZ.Factored","page":"ABCdeZ.jl","title":"ABCdeZ.Factored","text":"Factored{N} <: Distribution{Multivariate, MixedSupport}\n\nA Distribution type that can be used to combine multiple UnivariateDistribution's (independently).\n\nExamples\n\njulia> prior = Factored(Normal(0, 1), Uniform(-1, 1))\nFactored{2}(\np: (Normal{Float64}(μ=0.0, σ=1.0), Uniform{Float64}(a=-1.0, b=1.0))\n)\n\n\n\n\n\n","category":"type"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"pdf","category":"page"},{"location":"#Distributions.pdf","page":"ABCdeZ.jl","title":"Distributions.pdf","text":"pdf(d::Factored, x)\n\nFunction to evaluate the pdf of a Factored distribution object.\n\n\n\n\n\n","category":"function"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"logpdf","category":"page"},{"location":"#Distributions.logpdf","page":"ABCdeZ.jl","title":"Distributions.logpdf","text":"logpdf(d::Factored, x)\n\nFunction to evaluate the logpdf of a Factored distribution object.\n\n\n\n\n\n","category":"function"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"rand","category":"page"},{"location":"#Base.rand","page":"ABCdeZ.jl","title":"Base.rand","text":"rand(rng::AbstractRNG, factoreddist::Factored)\n\nFunction to sample one element from a Factored object.\n\n\n\n\n\n","category":"function"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"length","category":"page"},{"location":"#Base.length","page":"ABCdeZ.jl","title":"Base.length","text":"length(p::Factored)\n\nReturns the number of distributions contained in p.\n\n\n\n\n\n","category":"function"},{"location":"#Various-notes","page":"ABCdeZ.jl","title":"Various notes","text":"","category":"section"},{"location":"#ABC-Approximations","page":"ABCdeZ.jl","title":"ABC - Approximations","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"eps, summary stats","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"ABC (kernel instead of likelihood) and summary stats both introduce approximation errors, maybe read the  two lines in Didelot again...","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"eps trade off","category":"page"},{"location":"#More-on-model-evidences","page":"ABCdeZ.jl","title":"More on model evidences","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"when using summary stats => sufficient for model selection (link stackoverflow post and paper)","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"summary stats need to be sufficient for model selection        (it is not enough if summary stats are sufficient for         each model's parameters!), link to paper and         stackoverflow topic","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"\\@ref(normalisation_factor) off by normalisation factor (with default kernel), does not  matter when comparing models for the same eps;  what if same eps not available / impractical?","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"same ϵ target necessary (if not possible upper bound conservative        estimate may be possible, or, use ϵs and logZs lists for finding        last common ϵ to compare with)","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"explain here what to do when same eps difficult (link goes here...)","category":"page"},{"location":"#Features-for-the-distance-methods","page":"ABCdeZ.jl","title":"Features for the distance methods","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"In the distance function in the minimal example (dist!(θ, ve) = abs(model(θ)-data), nothing)  ve are \"external variables\" (varexternal in abcdesmc!) that can  be used in the distance computation and mutated in-place, even in the parallel mode  (each thread will obtain its own copy for thread-safe parallel ABC runs). ve is passed as 4th positional argument to abcdesmc! (nothing in the  minimal example).\nIn the distance function in the minimal example (dist!(θ, ve) = abs(model(θ)-data), nothing)  the second return argument (nothing) can be used to store arbitrary data  (blobs) to each particle; these blobs will be associated with the final  posterior samples/particles in the end. For example blobs could record the  actual simulation output:\nfunction dist2!(θ, ve, constants, data)\n    # constants can be used to pass thread-safe constants that are NOT mutated;\n    # ve for in-place, mutatable variables\n\n    # distance method\n    simdata = model(θ)\n    blob = simdata\n    d = abs(simdata-data)\n    d, blob\nend\ndist2!(θ, ve) = dist2!(θ, ve, nothing, data)\n\nr = abcdesmc!(prior, dist2!, ϵ, nothing, \n                    nparticles=1000, parallel=true)\n\nposterior = [t[1] for t in r.P[r.Wns .> 0.0]]\nevidence = exp(r.logZ)\nblobs = r.blobs[r.Wns .> 0.0]","category":"page"},{"location":"#References","page":"ABCdeZ.jl","title":"References","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"Some part of the code was copied, adapted and/or inspired by KissABC.jl [1]. For example,    the Factored syntax was adopted, abcdemc! is based on ABCDE, abcdesmc! is loosely based on    smc. We thank the developers of the package.\nA very good theory background for the general approach of model evidences from single ABC runs    is given by Didelot et al. (2011) [2]. More details on algorithms (in the likelihood-context) is found in Llorente et al. (2020) [3].\nThe differential evolution moves are introduced in Ter Braak (2006) [4].\nAs done also in KissABC.jl [1], the implementations of the abcdemc! method are a simplified    version of the method in Turner et al. (2012) [5]. The algorithmic idea in abcdesmc! is mostly based on Amaya et al. (2021) [6],   next to KissABC.jl, particular the handling of weights and the adaptive differential evolution move tuning (Amaya et al. (2021) is    in the likelihood context, which we adapted to ABC).\nCloser read on sufficient summary statistics for model comparison in ABC is found in Marin et al. (2014) [7] and condensed in this    stackexchange post [8].\nStratified resampling (for abcdesmc!) is inspired by Douc et al. (2005) [9].\nFor fast thread-based parallelism we make use of FLoops.jl [10].","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[1]: KissABC (https://github.com/francescoalemanno/KissABC.jl)","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[2]: Didelot et al. \"Likelihood-free estimation of model evidence.\" Bayesian Anal. 6 (1) 49 - 76, 2011.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[3]: Llorente et al. \"Marginal likelihood computation for model selection and hypothesis testing: an extensive review\" arXiv:2005.08334 [stat.CO], 2020.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[4]: Ter Braak. \"A Markov Chain Monte Carlo version of the genetic algorithm Differential Evolution: easy Bayesian computing for real parameter spaces\" Statistics and Computing volume 16, pages 239–249, 2006.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[5]: Turner et al. \"Approximate Bayesian computation with differential evolution\"  Journal of Mathematical Psychology, 2012.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[6]: Amaya et al. \"Adaptive sequential Monte Carlo for posterior inference and model selection among complex geological priors\" arXiv:2105.02012 [physics.geo-ph], 2021.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[7]: Marin et al. \"Relevant statistics for Bayesian model choice\" J. R. Statist. Soc. B, 2014.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[8]: https://stats.stackexchange.com/questions/26980/abc-model-selection","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[9]: Douc et al. \"Comparison of Resampling Schemes for Particle Filtering\" arXiv:cs/0507025 [cs.CE], 2005.","category":"page"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"[10]: FLoops (https://github.com/JuliaFolds/FLoops.jl)","category":"page"},{"location":"#Index","page":"ABCdeZ.jl","title":"Index","text":"","category":"section"},{"location":"","page":"ABCdeZ.jl","title":"ABCdeZ.jl","text":"","category":"page"}]
}
